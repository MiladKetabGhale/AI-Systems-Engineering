# Example configuration file for ETL and ML pipeline

# URL of the dataset to download (required)
data_url=https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.tgz

# Path where the raw data will be saved (optional; defaults to "default_data" if not specified)
data_path=datasets/housing_raw

# Path where the processed data will be saved (optional; defaults to "./cleanDatasets" if not specified)
save_path=processed_data/california_housing

# Specify the transformation module and class (these should align with the Makefile targets)
transformation_module=california_housing_transformation
transformation_class=CaliforniaHousingTransformation

# Training specifications
cv=5                          # Cross-validation folds; Required and must be >= 1

model=bagging                 # model name to use in training (mandatory)


ensemble_name=bagging         # Type of ensemble method (required if ensemble_training=True)


# Hyperparameters for ensemble method (only used if ensemble_training=True)
model_n_estimators=10
model_max_samples=0.8, 0.9    # Can specify ranges as comma-separated values for tuning with GridSearchCV
model_max_features=1.0, 0.7   # Can specify ranges as comma-separated values
model_bootstrap=True, False   # Can specify ranges for boolean values

# Hyperparameters for base model (only used if ensemble_training=False)
# Uncomment if not using ensemble and specify desired values
# base_model_tree_max_depth=None       # Maximum depth of the tree
# base_model_tree_min_samples_split=2  # Minimum samples required to split an internal node
# base_model_tree_min_samples_leaf=1   # Minimum samples required to be at a leaf node
# base_model_tree_max_features=None    # Number of features to consider when looking for the best split

# Additional paths (optional)
test_data_path=processed_data/california_housing/california_housing_prepared.csv
test_labels_path=processed_data/california_housing/california_housing_labels.csv

# later on specify the correct path for the testing. This is used for evaluating the trained model on the testing data
training_data_path=processed_data/california_housing/california_housing_prepared.csv
labels_data_path=processed_data/california_housing/california_housing_labels.csv



# Save predictive evaluations
save_predictive_evaluations=True

# =================================================================
# MODEL HYPERPARAMETERS FOR GRID SEARCH (ONLY IF ensemble_training=False)
# Uncomment and specify values if using any of the following models
# =================================================================

# ------------------------------
# Hyperparameters for Linear Regression
# ------------------------------
# base_model_linear_fit_intercept=True    # Whether to calculate the intercept
# base_model_linear_normalize=False       # Whether to normalize inputs (Deprecated)

# ------------------------------
# Hyperparameters for Lasso Regression
# ------------------------------
# base_model_lasso_alpha=1.0, 0.5, 0.1    # Constant that multiplies the L1 term
# base_model_lasso_max_iter=1000          # Maximum number of iterations
# base_model_lasso_tol=1e-4               # Tolerance for stopping criteria
# base_model_lasso_selection="cyclic"     # Selection of features ('cyclic' or 'random')

# ------------------------------
# Hyperparameters for Ridge Regression
# ------------------------------
# base_model_ridge_alpha=1.0, 0.1         # Regularization strength
# base_model_ridge_solver="auto"          # Solver to use ('auto', 'svd', 'cholesky', etc.)
# base_model_ridge_max_iter=1000          # Maximum number of iterations

# ------------------------------
# Hyperparameters for SGD Regressor
# ------------------------------
# base_model_sgdr_alpha=0.0001            # Regularization term
# base_model_sgdr_max_iter=1000           # Maximum number of iterations
# base_model_sgdr_tol=1e-3                # Tolerance for stopping criteria
# base_model_sgdr_learning_rate="optimal" # Learning rate schedule
# base_model_sgdr_eta0=0.01               # Initial learning rate for the 'constant' learning rate schedule

# ------------------------------
# Hyperparameters for Decision Tree Regressor
# ------------------------------
# base_model_tree_max_depth=None          # Maximum depth of the tree
# base_model_tree_min_samples_split=2     # Minimum samples required to split an internal node
# base_model_tree_min_samples_leaf=1      # Minimum samples required to be at a leaf node
# base_model_tree_max_features=None       # Number of features to consider when looking for the best split

# ------------------------------
# Hyperparameters for Random Forest Regressor
# ------------------------------
# base_model_forest_n_estimators=100      # Number of trees in the forest
# base_model_forest_max_depth=None        # Maximum depth of each tree
# base_model_forest_min_samples_split=2   # Minimum samples required to split an internal node
# base_model_forest_min_samples_leaf=1    # Minimum samples required to be at a leaf node
# base_model_forest_max_features="auto"   # Number of features to consider when looking for the best split
# base_model_forest_bootstrap=True        # Whether bootstrap samples are used when building trees

# ------------------------------
# Hyperparameters for SVR (Support Vector Regressor)
# ------------------------------
# base_model_svr_C=1.0                    # Regularization parameter
# base_model_svr_epsilon=0.1              # Epsilon in the epsilon-SVR model
# base_model_svr_kernel="rbf"             # Specifies the kernel type to be used in the algorithm
# base_model_svr_gamma="scale"            # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'

# ------------------------------
# Hyperparameters for Gradient Boosting Regressor
# ------------------------------
# base_model_gradient_boost_n_estimators=100  # Number of boosting stages to be run
# base_model_gradient_boost_learning_rate=0.1 # Learning rate shrinks the contribution of each tree
# base_model_gradient_boost_max_depth=3       # Maximum depth of each estimator

# ------------------------------
# Hyperparameters for XGBoost Regressor
# ------------------------------
# base_model_xgboost_n_estimators=100     # Number of boosting rounds
# base_model_xgboost_learning_rate=0.1    # Step size shrinkage
# base_model_xgboost_max_depth=3          # Maximum depth of each tree

# ------------------------------
# Hyperparameters for LightGBM Regressor
# ------------------------------
# base_model_lightgb_n_estimators=100     # Number of boosting rounds
# base_model_lightgb_learning_rate=0.1    # Step size shrinkage
# base_model_lightgb_num_leaves=31        # Maximum number of leaves in each tree

# =================================================================
# ENSEMBLE METHOD HYPERPARAMETERS (ONLY IF ensemble_training=True)
# Uncomment and specify values if using ensemble methods
# =================================================================

# ------------------------------
# Hyperparameters for Bagging Regressor
# ------------------------------
# ensemble_n_estimators=10                      # Number of base estimators in the ensemble
# ensemble_max_samples=1.0                      # Number of samples to draw from X to train each base estimator
# ensemble_max_features=1.0, 0.5                # Number of features to draw from X to train each base estimator
# ensemble_bootstrap=True                       # Whether samples are drawn with replacement

# ------------------------------
# Hyperparameters for AdaBoost Regressor
# ------------------------------
# ensemble_n_estimators=50                      # Maximum number of estimators
# ensemble_learning_rate=1.0                    # Weight applied to each regressor at each boosting iteration

# ------------------------------
# Hyperparameters for Gradient Boosting Regressor
# ------------------------------
# ensemble_n_estimators=100                     # Number of boosting stages
# ensemble_learning_rate=0.1                    # Learning rate shrinks the contribution of each tree
# ensemble_max_depth=3                          # Maximum depth of each tree

# ------------------------------
# Hyperparameters for XGBoost Regressor
# ------------------------------
# ensemble_n_estimators=100                     # Number of boosting rounds
# ensemble_learning_rate=0.1                    # Step size shrinkage
# ensemble_max_depth=3                          # Maximum depth of each tree

# ------------------------------
# Hyperparameters for LightGBM Regressor
# ------------------------------
# ensemble_n_estimators=100                     # Number of boosting rounds
# ensemble_learning_rate=0.1                    # Step size shrinkage
# ensemble_num_leaves=31                        # Maximum number of leaves in each tree
